{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math, time, os\n",
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "from torch.nn.attention import flex_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e28f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('autocomplete/lecture/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f'Length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f61f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "print(\"All the unique characters:\", ''.join(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deeaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder: string to integer\n",
    "def encode(string):\n",
    "    return [characters.index(c) for c in string]\n",
    "\n",
    "#decoder: integer to string\n",
    "def decode(index):\n",
    "  return ''.join([characters[i] for i in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56669cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "string = \"hello there\"\n",
    "print(encode(string))\n",
    "print(decode(encode(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05099891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b412d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "n = int(train_ratio * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae8c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64# how many independent sequences will we process in parallel?\n",
    "block_size = 256# what is the maximum context length for predictions?\n",
    "max_iters = 5000# how many batches to train on\n",
    "eval_interval = 500 # how often to evaluate the model\n",
    "learning_rate = 3e-4 # learning rate for optimizer\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "# device='cpu'\n",
    "eval_iters = 200 # how many batches to use for evaluation\n",
    "n_embd = 384 # embedding dimension\n",
    "n_head = 6 # number of attention heads\n",
    "n_layer = 6 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate\n",
    "sliding_window_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92b5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(is_train = True):\n",
    "    data = train_data if is_train else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # find batch_size random starting indices in the data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # get block_size length sequences starting from those indices\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # for testing, offset start index by 1 to preduct the next character\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a8d0887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tril(torch.ones(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b258a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "  def causal_mask(self, b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "  def __init__(self, head_size):\n",
    "    # self.device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "    self.device = device\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.o_proj = nn.Linear(head_size, n_embd, bias=False)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # self.block_mask = create_block_mask(self.causal_mask, 1, 1, block_size,block_size, device=self.device)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape # batch size, sequence length, embedding dimension (n_embd)\n",
    "    k = self.key(x)   # (B, T, head_size)\n",
    "    q = self.query(x)\n",
    "    \n",
    "    #compute attention scores\n",
    "    weights = torch.matmul(q, k.transpose(-2, -1)) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "    weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "    weights = self.dropout(weights)\n",
    "    \n",
    "    value = self.value(x) # (B, T, head_size)\n",
    "    # output = flex_attention.flex_attention(q, k, value,block_mask=self.block_mask)\n",
    "    # output = self.o_proj(output)\n",
    "    out = torch.matmul(weights, value) # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "    return out\n",
    "    # return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c24ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList(SelfAttentionHead(head_size) for _ in range(num_heads))\n",
    "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    out = self.dropout(self.proj(out))\n",
    "    return out\n",
    "\n",
    "class MultiHeadFlexAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout, device, sliding_window_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "        self.device = device\n",
    "\n",
    "        # define causal_mask inline or pass in\n",
    "        def causal_mask(b, h, q_idx, kv_idx):\n",
    "            return q_idx >= kv_idx\n",
    "          \n",
    "        def sliding_window(b,h,q_idx,kv_idx):\n",
    "          return q_idx-kv_idx <= sliding_window_len\n",
    "        \n",
    "        def mask(b,h,q_idx,kv_idx):\n",
    "          return causal_mask(b,h,q_idx,kv_idx) | sliding_window(b,h,q_idx,kv_idx)\n",
    "\n",
    "        # mask = torch.nn.attention.and_masks(causal_mask, sliding_window)\n",
    "        # self.register_buffer('causal_mask', mask)\n",
    "        self.block_mask = create_block_mask(mask, 1, 1, block_size, block_size, device=device)\n",
    "\n",
    "        # projections project full embedding into all heads\n",
    "        self.k = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.q = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.v = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.o_proj = nn.Linear(head_size * num_heads, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        nh, hs = self.num_heads, self.head_size\n",
    "\n",
    "        # project\n",
    "        k = self.k(x).view(B, T, nh, hs).transpose(1,2)\n",
    "        q = self.q(x).view(B, T, nh, hs).transpose(1,2)\n",
    "        v = self.v(x).view(B, T, nh, hs).transpose(1,2)\n",
    "\n",
    "        # crop mask to actual sequence length\n",
    "        print(T)\n",
    "        self.block_mask = self.block_mask._adjust(T, T)\n",
    "        print(self.block_mask.shape)\n",
    "\n",
    "        # call flex attention\n",
    "        out = flex_attention.flex_attention(q, k, v, block_mask=self.block_mask, enable_gqa=True)\n",
    "\n",
    "        # merge heads back\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, nh*hs)\n",
    "        out = self.o_proj(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47ed63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embd, 4 * n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embd, n_embd),\n",
    "        nn.Dropout(dropout),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "    \n",
    "class MoEFFN(nn.Module):\n",
    "  def __init__(self, n_embd, num_experts=4, num_experts_per_token=2):\n",
    "    super().__init__()\n",
    "    self.num_experts_per_token = num_experts_per_token\n",
    "    self.num_experts = num_experts\n",
    "    self.experts = nn.ModuleList([FFN(n_embd) for _ in range(num_experts)])\n",
    "    self.gate = nn.Linear(n_embd, num_experts)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    gate_scores = F.softmax(self.gate(x), dim=-1) # (B, T, num_experts)\n",
    "    expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1) # (B, T, C, num_experts)\n",
    "    # print(expert_outputs.shape, gate_scores.shape)\n",
    "    topk_scores, topk_indices = torch.topk(gate_scores, self.num_experts_per_token, dim=-1) # (B, T, 2)\n",
    "    top_probs = F.softmax(topk_scores, dim=-1) # (B, T, 2)\n",
    "    expert_outputs = torch.gather(expert_outputs, 3, topk_indices.unsqueeze(2).expand(-1, -1, C, -1)) # (B, T, C, 2)\n",
    "    out = (expert_outputs * top_probs.unsqueeze(2)).sum(dim=-1)  # (B, T, C)\n",
    "    return out\n",
    "\n",
    "class EfficientMoEFFN(nn.Module):\n",
    "    def __init__(self, n_embd, num_experts=4, num_experts_per_token=2):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_token = num_experts_per_token\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FFN(n_embd) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(n_embd, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(B*T, C)  # Flatten tokens to (batch*tokens, d_model)\n",
    "\n",
    "        # Gating\n",
    "        gate_scores = self.gate(x_flat)   # (B*T, num_experts)\n",
    "        topk_scores, topk_indices = torch.topk(\n",
    "            gate_scores, self.num_experts_per_token, dim=-1\n",
    "        )  # (B*T, k)\n",
    "        topk_probs = F.softmax(topk_scores, dim=-1)  # (B*T, k), normalized\n",
    "\n",
    "        # Output buffer\n",
    "        out = torch.zeros_like(x_flat)\n",
    "\n",
    "        # For each expert: route only the tokens assigned to it\n",
    "        for expert_id, expert in enumerate(self.experts):\n",
    "            # Find where this expert is selected\n",
    "            mask = (topk_indices == expert_id)  # (B*T, k)\n",
    "            if not mask.any():\n",
    "                continue # if it's not part of the top k selected experts for any token, skip it\n",
    "\n",
    "            token_ids, which_slot = mask.nonzero(as_tuple=True)\n",
    "\n",
    "            # Select actual tokens\n",
    "            tokens_for_expert = x_flat[token_ids]\n",
    "\n",
    "            # Apply expert FFN\n",
    "            expert_out = expert(tokens_for_expert)  # (num_tokens, C)\n",
    "\n",
    "            # Scale by probability\n",
    "            probs = topk_probs[token_ids, which_slot].unsqueeze(-1)\n",
    "            expert_out = expert_out * probs\n",
    "\n",
    "            # Scatter-add back to output buffer\n",
    "            out.index_add_(0, token_ids, expert_out)\n",
    "\n",
    "        return out.view(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a3ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  # block where you have mha and feedforward then layer normalization\n",
    "  def __init__(self, n_embd, n_head):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    # self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    self.sa = MultiHeadFlexAttention(n_head, head_size, n_embd, block_size, dropout, device, sliding_window_len)\n",
    "    # self.ffwd = FeedForward(n_embd)\n",
    "    self.ffwd =EfficientMoEFFN(n_embd, num_experts=4)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf50489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.token_embed_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.position_embed_table = nn.Embedding(block_size, n_embd)\n",
    "    self.blocks = nn.Sequential(\n",
    "      *[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "    token_emb = self.token_embed_table(idx) # (B, T, n_embd)\n",
    "    position_emb = self.position_embed_table(torch.arange(T, device=device))\n",
    "\n",
    "    x = token_emb + position_emb # (B, T, n_embd)\n",
    "    x = self.blocks(x) # (B, T, n_embd)\n",
    "    x = self.ln_f(x) # (B, T, n_embd)\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "    \n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # if(print_characters):\n",
    "      #   print(f\"\\r{decode(idx[-1].tolist())}\", end=\"\", flush=True) \n",
    "      # crop idx to the last block_size tokens\n",
    "      idx_cond = idx[:, -block_size:]\n",
    "      # get the predictions\n",
    "      logits, loss = self(idx_cond)\n",
    "      # focus only on the last time step\n",
    "      logits = logits[:, -1, :] # becomes (B, C)\n",
    "      # apply softmax to get probabilities\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # sample from the distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "      # append sampled index to the running sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "146e987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.064089 Million Model Parameters\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'Million Model Parameters')\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def generate_streaming(model, context, max_new_tokens):\n",
    "    max_new_tokens = max_new_tokens-1\n",
    "    for _ in range(max_new_tokens):\n",
    "      context = model.generate(context, max_new_tokens=1)\n",
    "      generated = decode(context[0].tolist())[-1]\n",
    "      print(f\"{generated}\", end=\"\", flush=True)\n",
    "    print()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8cedc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter 6/5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m x_data, y_data = get_batch(is_train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m optim.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m     14\u001b[39m position_emb = \u001b[38;5;28mself\u001b[39m.position_embed_table(torch.arange(T, device=device))\n\u001b[32m     16\u001b[39m x = token_emb + position_emb \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x) \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m     19\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     14\u001b[39m   x = x + \u001b[38;5;28mself\u001b[39m.sa(\u001b[38;5;28mself\u001b[39m.ln1(x))\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m   x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mEfficientMoEFFN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask.any():\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# if it's not part of the top k selected experts for any token, skip it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m token_ids, which_slot = \u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Select actual tokens\u001b[39;00m\n\u001b[32m     66\u001b[39m tokens_for_expert = x_flat[token_ids]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "  print(f\"\\r Iter {iter+1}/{max_iters}\", end=\"\", flush=True)\n",
    "  x_data, y_data = get_batch(is_train=True)\n",
    "  logits, loss = model(x_data, y_data)\n",
    "  optim.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "#train loop\n",
    "torch.save(model.state_dict(), 'autocomplete/models/model_v3_flex_attn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fea8e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('autocomplete/models/model_lecture_style.pth'))\n",
    "model.load_state_dict(torch.load('autocomplete/models/model_v3_flex_attn.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "416a6b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadFlexAttention' object has no attribute 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m context = torch.zeros((\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m), dtype=torch.long, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tolist()))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens)\u001b[39m\n\u001b[32m     37\u001b[39m idx_cond = idx[:, -block_size:]\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# get the predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m logits, loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# focus only on the last time step\u001b[39;00m\n\u001b[32m     41\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :] \u001b[38;5;66;03m# becomes (B, C)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m     14\u001b[39m position_emb = \u001b[38;5;28mself\u001b[39m.position_embed_table(torch.arange(T, device=device))\n\u001b[32m     16\u001b[39m x = token_emb + position_emb \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x) \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[32m     19\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m   x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m   x = x + \u001b[38;5;28mself\u001b[39m.ffwd(\u001b[38;5;28mself\u001b[39m.ln2(x))\n\u001b[32m     16\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mMultiHeadFlexAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     49\u001b[39m v = \u001b[38;5;28mself\u001b[39m.v(x).view(B, T, nh, hs).transpose(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m T < block_size:\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# make a tight mask for small context\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     block_mask = create_block_mask(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, T, T, device=x.device)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# use precomputed mask and _adjust for efficiency\u001b[39;00m\n\u001b[32m     56\u001b[39m     block_mask = \u001b[38;5;28mself\u001b[39m.block_mask._adjust(T, T)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/jupyter/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1962\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1960\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1961\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1963\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1964\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiHeadFlexAttention' object has no attribute 'mask'"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cb97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "The first sends between the warld of itself,\n",
      "That the prepared for my kinsmen's delight,\n",
      "Re-quicken'd her shore, her on God,\n",
      "Incapable to an impeach shall my cherish here\n",
      "A mile richest shepherd.\n",
      "\n",
      "BUSHY:\n",
      "Cousin, I pray you, lords; and this grown business\n",
      "It would but see term than my reported:\n",
      "Such that in slain them restored me wrath\n",
      "Thus hurrip'd with magnany a dish death\n",
      "Throw upon thy house.\n",
      "\n",
      "RICHMOND:\n",
      "It is my lady; I'll bear weeping you to the grave:\n",
      "Then but valiant first and witeight.\n",
      "But, love, go, come by Minitio.\n",
      "Hasting title our san;\n",
      "Say is better 's un gravator greypard's sains;\n",
      "There's no more of it. Pray you, s\n",
      "Havi been your humility, and you deliver\n",
      "Margary to sanctuary fortune, and chaste\n",
      "That murdering your handship subject?\n",
      "For sorrow'st of royalive, when I loved how;\n",
      "I'll follow me for what I hammerchanced an happy;\n",
      "For I will give you on his knees.\n",
      "\n",
      "NORFOLOLANUS:\n",
      "Mark, my sovereign neighbours, and theirs lie,\n",
      "His answer'd his hands: in a great day for Roques,\n",
      "So 'bark,--\n",
      "\n",
      "MONTAGUE:\n",
      "My gracious brother,\n",
      "Renowned to them and I am now:\n",
      "By whom comfortant is a crefer--peacell'd an unmass,\n",
      "With depart, and tany lowless ampenss made\n",
      "Doth hurry what I inform'd there?\n",
      "O but deadly, here a entreators!\n",
      "If answer I will have loved my exclaim,\n",
      "I dare not be not afrant touch my counsel,--thy mouth\n",
      "For my condage to thee. if Berke, God save thee.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Hadst thou undon't, and my hands I thought,--\n",
      "\n",
      "KING RICHARD III:\n",
      "Cousin of Richard, and planet, for what camest.\n",
      "God God I bend abhor, and I friend to our clear;\n",
      "And so obstain'd in harritary here been!\n",
      "Remon so weary.\n",
      "\n",
      "Black again:\n",
      "But Richmond have prevend that made an once.\n",
      "Thou shalt not hear me.\n",
      "\n",
      "CLIFFORD:\n",
      "A little foe so? or both great told marre us\n",
      "No beeneral maid wide upon their people.\n",
      "\n",
      "Hastens:\n",
      "Take my counsel thy burial cousin, the charm,\n",
      "As die upon my tongue down, and thy stooping then.\n",
      "To God counsel my frownwas at unto you;\n",
      "Withh his county man perspect my son, good men.\n",
      "\n",
      "SICINIUS:\n",
      "A minutoring here?\n",
      "\n",
      "BRUTUS:\n",
      "And, holy life way upon his; from his gravish:\n",
      "Cold tages spident, and purple curb.\n",
      "\n",
      "BRUTUS:\n",
      "My sacrificious sovews us usberved!\n",
      "\n",
      "LEONTES:\n",
      "Would return to your houses! Sir, our wisdom held!\n",
      "Bear not our company\n",
      "With all the monument of your actions.\n",
      "\n",
      "MAMILLLO:\n",
      "Nay, but, and boil't, my daughter's plainness;\n",
      "My lord; young life bread or debt. This loss\n",
      "I proternoon, to detestion\n",
      "And make my men full another poor and guilty\n",
      "Call Warwick gonce had for't in due bninession,\n",
      "Which he, ever their caps our mile. His noble spirity--a\n",
      "importance my prophecy and the lustful sword!\n",
      "If ever I crept now in an him. Withdraw already!\n",
      "God save your beasts! I am sworn by oath grown;\n",
      "My father prince is gone. O bring to my serious-days!\n",
      "Speak so desperseth but to me a land,\n",
      "The thoughts; yet, ther is profit to devise,\n",
      "And with death's bond, with some swiftnessith wing,\n",
      "As coward down, I four meet will play the wars;\n",
      "And when he had given him follow'd and for onces,\n",
      "Our strength shadow turns to burnsble their own,\n",
      "Commit their love; often kissly me,\n",
      "And hear bold it in the nursty peer; gave me\n",
      "Thy sisters, and do I lose thy consent\n",
      "The way are freshed me fled sheerer. What!\n",
      "They shall rensolt repaid, if thou hast\n",
      "A humbly shadow like, ambitive like thousance,\n",
      "Ere so cunnible not?\n",
      "\n",
      "ISABELLA:\n",
      "She is all, whom on his repairing, were to melt\n",
      "The other people easy to see, or I oed,\n",
      "Was certainly like cup all'd in my daughter.\n",
      "Away! if I do beseem troth, parcell'd the locky presence\n",
      "Wept that out of kings against guilty.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "How now! mast I mean, but confess me with,\n",
      "Or many fearing requess, spacious in wars,\n",
      "And flesh whosoever of Rosaline mark'd\n",
      "Upon the polscion. Be that is it?\n",
      "\n",
      "SICINIUS:\n",
      "A prayel\n",
      "My old peaces speaks moke things: here comes the\n",
      "lord preparations,\n",
      "Then that we have leaded your idlerity,\n",
      "And by one that the precious should enjoy.\n",
      "\n",
      "SICINIUS:\n",
      "An have driver, harm, but you holy hour\n",
      "And enforct or your noble uncleance\n",
      "To meeet him to answer me, and mock'd\n",
      "How now you chose your business. Come, let us beg our\n",
      "but twenty; we have begun to come and mystery;\n",
      "You have been even upon his throne;\n",
      "Whate'er as you are, but one he; say so, your care\n",
      "Her nurrish'd his son visits me to death,\n",
      "To have the heir present that you composition;\n",
      "But, like you, and odd: he stood he and close.\n",
      "\n",
      "LUCENTIO:\n",
      "Takes him here, be done? tell me; but, good do.\n",
      "\n",
      "POMPE:\n",
      "Though old odd man, and neither talk,\n",
      "I made my father, proud be the falchish hat,\n",
      "Being her, and I'll watch yiel.\n",
      "\n",
      "CAMILLO:\n",
      "My heart company out for your honours,\n",
      "Mistake to the frownright of justice store.\n",
      "\n",
      "LEONTES:\n",
      "You would have made, but you have ant,\n",
      "That has given like hands painted, and my service\n",
      "Will nothing imprisonment. If you'll have likely,\n",
      "Look you there be knock'd on this scorn, observatory,\n",
      "Cuttering off this rigour, surrent not this\n",
      "Which you vow or find, but we let hell burn,\n",
      "His oathly fant: therefore, none a care womany\n",
      "country fallibly, and a gentle soul.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_be_continued = \"ROMEO\"\n",
    "context = torch.tensor(encode(text_to_be_continued), dtype=torch.long, device=device).unsqueeze(0)\n",
    "# print(decode(model.generate(context, max_new_tokens=5000)[0].tolist()))\n",
    "# model.generate(context, max_new_tokens=50,print_characters=True)\n",
    "generate_streaming(model, context, max_new_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf41191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
