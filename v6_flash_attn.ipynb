{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math, time, os\n",
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "from torch.nn.attention import flex_attention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e28f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('autocomplete/lecture/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f'Length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f65d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12357aca8204a5fbd7c95143c4f1a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f20efa9cb341049775eeeb40c5592e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her campaign emailed a fundraising pitch Tuesday evening warning of the dangers of a Trump presidency and of complacency among Democrats.\n",
      "{'text': \"Canonical, keeper of the Ubuntu Linux distribution, is a small company with big friends. The latest example: Dell, IBM and Intel each are taking new steps with Ubuntu. Here's the scoop.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "dataset = load_dataset(\"Bingsu/openwebtext_20p\")\n",
    "# This gives you cleaned, plain text articles1\n",
    "print(dataset['train'][100]['text'][:500])  # Print the first 500 characters of the first article\n",
    "print(dataset['train'][600000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f61f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "print(\"All the unique characters:\", ''.join(characters))\n",
    "vocab_size = tiktoken.get_encoding(\"gpt2\").n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4deeaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder: string to integer\n",
    "def encode(string):\n",
    "    return tiktoken.get_encoding(\"gpt2\").encode(string)\n",
    "    # return [characters.index(c) for c in string]\n",
    "\n",
    "#decoder: integer to string\n",
    "def decode(index):\n",
    "  return tiktoken.get_encoding(\"gpt2\").decode(index)\n",
    "  # return ''.join([characters[i] for i in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56669cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 612]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "string = \"hello there\"\n",
    "print(encode(string))\n",
    "print(decode(encode(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05099891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338025]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b412d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([304222]) torch.Size([33803])\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "n = int(train_ratio * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae8c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# batch_size =16# how many independent sequences will we process in parallel?\n",
    "# block_size = 256# what is the maximum context length for predictions?\n",
    "# max_iters = int(5000* 64/batch_size) # how many batches to train on\n",
    "# eval_interval = 500 # how often to evaluate the model\n",
    "# learning_rate = 3e-4 # learning rate for optimizer\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "# eval_iters = 200 # how many batches to use for evaluation\n",
    "# n_embd = 384 # embedding dimension\n",
    "# n_head = 6 # number of attention heads\n",
    "# n_layer = 6 # number of transformer blocks\n",
    "# dropout = 0.2 # dropout rate\n",
    "# sliding_window_len =64 \n",
    "# hyperparameters\n",
    "load_previous = False\n",
    "save_on_interrupt=True\n",
    "train_model =False\n",
    "batch_size =24# how many independent sequences will we process in parallel?\n",
    "block_size =256# what is the maximum context length for predictions?\n",
    "max_iters = int(5000* 64/batch_size) # how many batches to train on\n",
    "eval_interval = 500 # how often to evaluate the model\n",
    "learning_rate = 3e-4 # learning rate for optimizer\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "eval_iters = 200 # how many batches to use for evaluation\n",
    "n_embd = 384# embedding dimension\n",
    "n_head = 6# number of attention heads\n",
    "n_layer = 6 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate\n",
    "sliding_window_len =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92b5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch(is_train = True):\n",
    "#     data = train_data if is_train else val_data\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     # find batch_size random starting indices in the data\n",
    "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "#     # get block_size length sequences starting from those indices\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "#     # for testing, offset start index by 1 to preduct the next character\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n",
    "num_dataset_articles = len(dataset['train'])\n",
    "def get_batch(is_train = True):\n",
    "    data = dataset['train'] if is_train else dataset['validation']\n",
    "    ix = torch.randint(num_dataset_articles, (batch_size,)).view(batch_size,)\n",
    "    # print(ix)\n",
    "    # find batch_size random starting indices in the data\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in ix:\n",
    "        # print(f\"ix: {i}\")\n",
    "        # print(data[100])\n",
    "        i = int(i)\n",
    "        # print(data[i])\n",
    "        # print(data[i]['text'][:100])\n",
    "        article = data[i]['text']\n",
    "        # print(article[:100])\n",
    "        article_ids = encode(article)\n",
    "        while len(article_ids) < block_size + 2:\n",
    "            article_ids = encode(article+data[torch.randint(num_dataset_articles, (1,)).item()]['text'])\n",
    "        # print(f\"len article ids: {len(article_ids)-block_size-1}\")\n",
    "        start_idx = torch.randint(0, len(article_ids) - block_size - 1, (1,)).item()\n",
    "        x_list.append(torch.tensor(article_ids[start_idx:start_idx+block_size], dtype=torch.long))\n",
    "        y_list.append(torch.tensor(article_ids[start_idx+1:start_idx+block_size+1], dtype=torch.long))\n",
    "    x = torch.stack(x_list)\n",
    "    y = torch.stack(y_list)\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be107003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        # just set a large virtual length\n",
    "        return 10**9  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = get_batch(is_train=self.is_train)\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TrainDataset(is_train=True),\n",
    "    batch_size=None,        # because get_batch already gives a full batch\n",
    "    num_workers=4,          # number of parallel workers (tune to your CPU)\n",
    "    prefetch_factor=2,      # workers preload ahead\n",
    "    pin_memory=True         # speeds up transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8d0887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tril(torch.ones(9,9)) - torch.tril(torch.ones(9,9), -2))\n",
    "print(torch.tril(torch.ones(9,9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b258a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "  def causal_mask(self, b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "  def __init__(self, head_size):\n",
    "    self.device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.o_proj = nn.Linear(head_size, n_embd, bias=False)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)- torch.tril(torch.ones(block_size, block_size), -sliding_window_len)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # self.block_mask = create_block_mask(self.causal_mask, 1, 1, block_size,block_size, device=self.device)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape # batch size, sequence length, embedding dimension (n_embd)\n",
    "    k = self.key(x)   # (B, T, head_size)\n",
    "    q = self.query(x)\n",
    "    \n",
    "    #compute attention scores\n",
    "    weights = torch.matmul(q, k.transpose(-2, -1)) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "    weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "    weights = self.dropout(weights)\n",
    "    \n",
    "    value = self.value(x) # (B, T, head_size)\n",
    "    # output = flex_attention.flex_attention(q, k, value,block_mask=self.block_mask)\n",
    "    # output = self.o_proj(output)   \n",
    "    out = torch.matmul(weights, value) # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "    return out\n",
    "    # return output\n",
    "\n",
    "class FlashAttentionHead(nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    # self.device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.o_proj = nn.Linear(head_size,head_size, bias=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    #given: 6,512,4096 -> 3072, 4096. want: 512, 512\n",
    "    B, T, C = x.shape # batch size, sequence length, embedding dimension (n_embd)\n",
    "    # print(B, T, C)\n",
    "    k = self.key(x)   # (B, T, head_size)\n",
    "    q = self.query(x)\n",
    "    \n",
    "    value = self.value(x) # (B, T, head_size)\n",
    "    output = F.scaled_dot_product_attention(q, k, value, attn_mask=None, dropout_p=dropout, is_causal=True)\n",
    "    # print(output.shape)\n",
    "    # print(self.o_proj(output).shape)\n",
    "    output = self.o_proj(output)\n",
    "    output = self.dropout(output)\n",
    "    return output\n",
    "    # return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c24ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList(FlashAttentionHead(head_size) for _ in range(num_heads))\n",
    "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    # print(f\"out shape before proj: {out.shape}\\n\")\n",
    "    # print(f\"out shape after concat: {self.proj(out).shape}\\n\")\n",
    "    out = self.dropout(self.proj(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f47ed63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embd, 4 * n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embd, n_embd),\n",
    "        nn.Dropout(dropout),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "    \n",
    "class MoEFFN(nn.Module):\n",
    "  def __init__(self, n_embd, num_experts=4, num_experts_per_token=2):\n",
    "    super().__init__()\n",
    "    self.num_experts_per_token = num_experts_per_token\n",
    "    self.num_experts = num_experts\n",
    "    self.experts = nn.ModuleList([FFN(n_embd) for _ in range(num_experts)])\n",
    "    self.gate = nn.Linear(n_embd, num_experts)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    gate_scores = F.softmax(self.gate(x), dim=-1) # (B, T, num_experts)\n",
    "    expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1) # (B, T, C, num_experts)\n",
    "    # print(expert_outputs.shape, gate_scores.shape)\n",
    "    topk_scores, topk_indices = torch.topk(gate_scores, self.num_experts_per_token, dim=-1) # (B, T, 2)\n",
    "    top_probs = F.softmax(topk_scores, dim=-1) # (B, T, 2)\n",
    "    expert_outputs = torch.gather(expert_outputs, 3, topk_indices.unsqueeze(2).expand(-1, -1, C, -1)) # (B, T, C, 2)\n",
    "    out = (expert_outputs * top_probs.unsqueeze(2)).sum(dim=-1)  # (B, T, C)\n",
    "    return out\n",
    "\n",
    "class EfficientMoEFFN(nn.Module):\n",
    "    def __init__(self, n_embd, num_experts=4, num_experts_per_token=2):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_token = num_experts_per_token\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FFN(n_embd) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(n_embd, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(B*T, C)  # Flatten tokens to (batch*tokens, d_model)\n",
    "\n",
    "        # Gating\n",
    "        gate_scores = self.gate(x_flat)   # (B*T, num_experts)\n",
    "        topk_scores, topk_indices = torch.topk(\n",
    "            gate_scores, self.num_experts_per_token, dim=-1\n",
    "        )  # (B*T, k)\n",
    "        topk_probs = F.softmax(topk_scores, dim=-1)  # (B*T, k), normalized\n",
    "\n",
    "        # Output buffer\n",
    "        out = torch.zeros_like(x_flat)\n",
    "\n",
    "        # For each expert: route only the tokens assigned to it\n",
    "        for expert_id, expert in enumerate(self.experts):\n",
    "            # Find where this expert is selected\n",
    "            mask = (topk_indices == expert_id)  # (B*T, k)\n",
    "            if not mask.any():\n",
    "                continue # if it's not part of the top k selected experts for any token, skip it\n",
    "\n",
    "            token_ids, which_slot = mask.nonzero(as_tuple=True)\n",
    "\n",
    "            # Select actual tokens\n",
    "            tokens_for_expert = x_flat[token_ids]\n",
    "\n",
    "            # Apply expert FFN\n",
    "            expert_out = expert(tokens_for_expert)  # (num_tokens, C)\n",
    "\n",
    "            # Scale by probability\n",
    "            probs = topk_probs[token_ids, which_slot].unsqueeze(-1)\n",
    "            expert_out = expert_out * probs\n",
    "\n",
    "            # Scatter-add back to output buffer\n",
    "            out.index_add_(0, token_ids, expert_out)\n",
    "\n",
    "        return out.view(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a3ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  # block where you have mha and feedforward then layer normalization\n",
    "  def __init__(self, n_embd, n_head):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    # self.ffwd = FeedForward(n_embd)\n",
    "    self.ffwd =EfficientMoEFFN(n_embd, num_experts=4)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(f\"x shape: {x.shape}\\n sa,ln1 shape: {self.sa(self.ln1(x)).shape}\\n ffwd,ln2 shape: {self.ffwd(self.ln2(x)).shape}\\n \")\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf50489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.token_embed_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.position_embed_table = nn.Embedding(block_size, n_embd)\n",
    "    # self.rotary_emb = RotaryPositionalEmbedding(n_embd)\n",
    "    self.blocks = nn.Sequential(\n",
    "      *[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "    token_emb = self.token_embed_table(idx) # (B, T, n_embd)\n",
    "    position_emb = self.position_embed_table(torch.arange(T, device=device))\n",
    "    # rotary_emb = self.rotary_emb(T, device=device)\n",
    "\n",
    "    x = token_emb + position_emb # (B, T, n_embd)\n",
    "    # print(f\"shape rotary emb: {rotary_emb.shape}\")\n",
    "    # rotary_emb = rotary_emb.unsqueeze(0).expand(B, -1, -1)  # (1, T, n_embd) -> (B, T, n_embd)\n",
    "    # print(f\"shape rotary emb after unsqueeze and expand: {rotary_emb.shape}\")\n",
    "    # print(f\"tokenembed shape: {token_emb.shape}\")\n",
    "    # x = token_emb * rotary_emb\n",
    "    x = self.blocks(x) # (B, T, n_embd)\n",
    "    x = self.ln_f(x) # (B, T, n_embd)\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "    \n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens, print_characters=False, temperature=1, top_k=50, greedy=False):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, loss = self(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        if greedy:\n",
    "            idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if top_k is not None:\n",
    "                values, indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "                probs_filtered = torch.zeros_like(probs).scatter_(-1, indices, values)\n",
    "                probs = probs_filtered / probs_filtered.sum(dim=-1, keepdim=True)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        if print_characters:\n",
    "            print(decode(idx[0, -1:].tolist()), end=\"\", flush=True)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "146e987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.811497 Million Model Parameters\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'Million Model Parameters')\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def generate_streaming(model, context, max_new_tokens):\n",
    "    max_new_tokens = max_new_tokens-1\n",
    "    for _ in range(max_new_tokens):\n",
    "      context = model.generate(context, max_new_tokens=1)\n",
    "      generated = decode(context[0].tolist())[-1]\n",
    "      print(f\"{generated}\", end=\"\", flush=True)\n",
    "    print()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8cedc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(load_previous):\n",
    "#   model.load_state_dict(torch.load(\"autocomplete/models/model_v6_flash_attn.pth\"))\n",
    "# for iter in range(max_iters):\n",
    "#   x_data, y_data = get_batch(is_train=True)\n",
    "#   logits, loss = model(x_data, y_data)\n",
    "#   optim.zero_grad(set_to_none=True)\n",
    "#   loss.backward()\n",
    "#   optim.step()\n",
    "#   print(f\"\\r Iter {iter+1}/{max_iters}, Loss {loss.item()}\", end=\"\", flush=True)\n",
    "# #train loop\n",
    "# torch.save(model.state_dict(), 'autocomplete/models/model_v6_flash_attn.pth')\n",
    "if train_model:\n",
    "  try:\n",
    "      torch.set_float32_matmul_precision('high')\n",
    "      if load_previous:\n",
    "          model.load_state_dict(torch.load(\"autocomplete/models/model_v6_flash_attn.pth\"))\n",
    "\n",
    "      compiled_model=torch.compile(model)  \n",
    "\n",
    "      data_iter = iter(train_loader)\n",
    "      for iter in range(max_iters):\n",
    "          # x_data, y_data = get_batch(is_train=True)\n",
    "          x_data, y_data = next(data_iter)\n",
    "          x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "          logits, loss = compiled_model(x_data, y_data)\n",
    "\n",
    "          optim.zero_grad(set_to_none=True)\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "\n",
    "          print(f\"\\r Iter {iter+1}/{max_iters}, Loss {loss.item()}\", end=\"\", flush=True)\n",
    "      torch.save(model.state_dict(), \"autocomplete/models/model_v6_flash_attn.pth\")\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "      if save_on_interrupt:\n",
    "        print(\"\\nTraining interrupted by user. Saving model...\")\n",
    "      else:\n",
    "        print(\"\\nTraining interrupted by user. Model not saved.\")\n",
    "\n",
    "  finally:\n",
    "    if save_on_interrupt:\n",
    "      torch.save(model.state_dict(), \"autocomplete/models/model_v6_flash_attn.pth\")\n",
    "      print(\"\\nModel state saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fea8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('autocomplete/models/model_lecture_style.pth'))\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model.load_state_dict(torch.load('autocomplete/models/model_v6_flash_attn.pth'))\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "416a6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He will not hesitate. You will be the chief minister of the country, one of the most vocal of the people of the country.����We are waiting for this time for that. We are waiting for that moment of this moment of this year that we will announce what we are going to talk about, not just in our life and at night we are getting ready ahead.��The British press published its comments in 2012, under the headline ��My Little Pony is on My Little Pony: Friendship is Magic��. The following words will be published in the UK.One of the most well-known cartoons to be published and the last from people who were invited by the show��s sponsors were announced over the phone. According to the site, there will be a number of cartoonists, about half of them are ��people�� to the show, about seven times a minute.In October 2012, it was announced that this type of photo-taking was going on, a new report was posted on the popular media website, ��The Star Wars Blog��.The official publication of the official Daily Stormer��s comments was repeated by several independent sources on June 11, 2012 in China and has already been published.In December 2012, it appeared that the UK��s Ministry of Tourism stated that ��The number�� was ��an obvious result of propaganda��.This new version of the story seems to have been taken from the post, which has been published since the internet was published on June 1, 2012.The new information is here on www.–Facebook.com.In the past they released images shared by the BBC and its parent the same day: (accessed January 1, 2012) to the press of the Daily Stormer. This image appears to be the same as the original article in the Daily Stormer��s Daily Stormer magazine.The media media of the story will be removed from the official newspaper. Also notice that the article is not official. A photo in the news has to be re-released by the Daily Stormer.It is not clear if the images had been used to express the article will be returned by the media.The official report from the Australian newspaper, \"The Star Wars Journalism Foundation Project (Penguins) has seen the official versions of the original article in the newspaper.According to the Post it was made with the news, published by CNN for the first! He will not hesitate. You will be the chief minister of the country, one of the most vocal of the people of the country.”“We are waiting for this time for that. We are waiting for that moment of this moment of this year that we will announce what we are going to talk about, not just in our life and at night we are getting ready ahead.”The British press published its comments in 2012, under the headline “My Little Pony is on My Little Pony: Friendship is Magic”. The following words will be published in the UK.One of the most well-known cartoons to be published and the last from people who were invited by the show’s sponsors were announced over the phone. According to the site, there will be a number of cartoonists, about half of them are “people” to the show, about seven times a minute.In October 2012, it was announced that this type of photo-taking was going on, a new report was posted on the popular media website, “The Star Wars Blog”.The official publication of the official Daily Stormer’s comments was repeated by several independent sources on June 11, 2012 in China and has already been published.In December 2012, it appeared that the UK’s Ministry of Tourism stated that “The number” was “an obvious result of propaganda”.This new version of the story seems to have been taken from the post, which has been published since the internet was published on June 1, 2012.The new information is here on www.–Facebook.com.In the past they released images shared by the BBC and its parent the same day: (accessed January 1, 2012) to the press of the Daily Stormer. This image appears to be the same as the original article in the Daily Stormer’s Daily Stormer magazine.The media media of the story will be removed from the official newspaper. Also notice that the article is not official. A photo in the news has to be re-released by the Daily Stormer.It is not clear if the images had been used to express the article will be returned by the media.The official report from the Australian newspaper, \"The Star Wars Journalism Foundation Project (Penguins) has seen the official versions of the original article in the newspaper.According to the Post it was made with the news, published by CNN for the first\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500, print_characters=True)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799af6d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "903cb97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��, we must remember that this new environment must have taken place at the beginning of this first phase of history. Since it is in a world of constant flux, the past and the future are not even close together, but since we are in a world where most of the human civilizations may suffer from the same fate as life itself, we will cease our efforts to be alive at the beginning of this final phase of the human society; the ancient empire that is one of light and darkness in its way can be restored; and the planet that��s darkness and light shining in the heavens! This kind of world is more of a science, an example of the modern era than most human society ever conceived. In the past, we were always imagining more of these new worlds being in the range of the past. The human civilization, in other words, became an order of dark forces of the past: the future and the end of that mind that is the final and final time for us to live. We never think this way, yet we always remember the past, but to that there are still people with whom it will be in the midst of the future, it was not as good in the present moment. This is why we are at the beginning of this new era. We are at what point, the beginning of this new age of humanity��s future, in a world where people of us will look and see the world as in the future; but the experience of our journey could not be taken away, and not, but I also want to say this: for we��re at the beginning of this epoch of civilization- we can all share the same fate. I think that our next generation of our lives will become our ��world�� in many ways; we can ever know that we have the opportunity to live more as a country or a place to be as good as all else. But we can��t exist. We can��t. And we can have the people who care about us. We can live better when we can and should be treated equally. If we don��t, or we have it, we can��t. And don��t. We can��t, and we can��t, or if we don��t, it should be possible to live on it now. And we can never know how to survive without being on our feet. We can��t, we can��t, but if we��re not, we don��t have it. We can��t live in a world that they can��t, because the government has no moral right to be respected, because we can��t and no human beings can get there. I think we should have our rights. Let the people, and we can keep the money, and the people, we can live free if they can��t, I don��t know what they��ve done so far, but I don��t want to be a man. Maybe you want to be your own brother, so what��s that? Would you do that in your name if I��m going to live life unless I��m in a fight? And I��m just gonna be a man I just don��t know. And I want to be a man, I��m gonna have everything in my life. And then I��ll be the father of his children, all the way to Earth. But I��m gonna be a man to me, and hopefully my family and I��ll be a man to me. And I will be there again. I��ll be in a room with all my children, every night. Let me go, my family��s, the son, and my kids. And I��ll be there again.�� And we��ll share a great mutual friend. He��s not afraid of him because of the money he��s having to offer it to him and have him put up there. I felt like my father who��s paying it to him can��t answer this question. I get a little less of an explanation for what��s happening, which is a very good thing, there��s a great deal of love in him, other than more love. So, I don��t know if we��ll ever know it, because I think it��s the same thing as I have a friend and a son because you might want a lot of love, and that��s the real deal, to all his friends, and they do. Then, we��ll have a different story, a relationship with him, each about himself, and we won��t talk about one another—not on the same topic—in the next room or in the next room, with a child, so on. It��s not really important to me, because I should do that and that��s what I do. It��s because I��m a lot of people I feel comfortable with for other purposes. I was a kid who��s very, very protective of what��s going on and so forth.�� [He��s] thinking about it over a long time is going to give away. He is being kind of frustrated, it��s because nobody has done it, unless the child is like, ��Oh my God, what happens when you��re angry?�� We��d be like, ��Fuck, I��m in a dream, you know,�� we have, so that means that we��re really just seeing different ends and there��s no point in saying that we get to make that relationship. And that��s when it��s time to go away. We��ll come out in a ��my brother�� and when we��re like, ��You��re doing something,�� the idea of that moment is the thing I��m watching on the show and I like it because it��s such a little weird. And we��d be very kind of happy to make this way easier in a different way. You can tell that we��ve seen similar ends but the thing about [The Dnyskorn] the way a lot of things are going on are like, ��Just see how he��s going.�� And then we��ve seen an improvement from, ��This is funny.�� That��s what he��s doing, and how it can change the mind of what��s going on. And we��ve seen a decrease in the amount of time that��s going on. Just look at the show and see what happens next. And there��s an awful disconnect between two different ends. And you never know how much that��s gonna happen to an actor who��s not the person who��s a person, but you have a different way of doing it. Because for me, it��s the same sort of a situation that��s been so many [laughs]. I��ve gotten it myself. I mean, this has become so much worse. It��s like, ��Yeah, I know how much that happened.�� It��s a hard time to even learn. I��m not a good person that comes across as the only person in the show. When we talk about it, you see it, ��It can��t happen again,�� which is really the same story. I don��t have to learn. I��ve heard that, because after reading the first episode, I would never get it. We have to get you back on track at some point just because you��re looking at me like, Why is there something that I think? You��ve been looking forward. It doesn��t turn out the direction I did a couple years ago. [Laughter.] And I��m really interested in thinking about this show, because it looks like it��s going to show when I get back to the end, because if I lost it he��s doing it now. I��m excited because I��ve been getting the season five episode for two decades, but then, I do see that because when I see something big I do it, I��m ready to move on, so I��m ready to do it. I��m gonna do it now, because that would give me that time and have a sense of responsibility.�� That��s something that I��m really proud of in its life. It��s been a lot in my life and it��s kind of the best way there��s not a lot of people, you don��t know where it��s going from there. And I think it might be a little bit more of a bit more of a bit more time. Right now my season has a few really good guys. So it��s been fun to watch these people play their games more than once, but I have to be a year and I��m excited to see all the new kid from the back on the couch with the big picture of my kids playing at home. It��s a big deal because I can��t play in my life, or anything like that unless it��s a long play. But the time is finally there and it goes the same way as it was before I can use it. That��s part of why I go this long and that��s actually where I��m most proud of but still.��You have said I need your input and it��s time to build that one or the other,�� he said. ��Don��t let it go wrong. It��s a good chance for your team to be better and better and better and see the better or win faster as well.�� They��ve been more than happy. The same time, on the left there��s a lot of hope about the teams you need. They��ve been a big part of the team around town, and I don��t really know if they��ll want to stay in town like they did before that before the season��s events.��I remember thinking about the game, about 20 of our kids playing against three of us,�� he added. ��The team in Dallas, I feel really good. I��m thrilled to be a part of our team.��The second of those? Not long after we was announced, it was announced that an agreement was reached, which means we��ll need to pay $18 million to come back to Toronto in the 2013, and that��s a $11 million deal, yet we��ll be happy to see if he can help us make the change and we can say, oh.�� That��s exactly what we��ve tried to do. The team doesn��t, and it doesn��t take an extra one��s. One way to say that we��re all on our side is to see the team and the media? Maybe it��s very much about our own history since the start of it.��Meanwhile, on Wednesday, and Monday, they went from having the option, though the club wasn��t happy watching it. It should be expected to be able to use the two-point effort without one of these two-point games on the table to give them the best shot at the championship — and that means that for us that has to be an important step in life for us it��s a great opportunity.��We��ll get a little more time, but I��ve been back with some friends on the board,�� said James Binder, the player and the owner of the club. ��We��re so excited to see the game, and I��ve been wanting it. I��m looking forward to working with a little bit on my hands,\" says the former player. ��And of course, that is a big challenge to have. It��s an important component of the coaching of people and I��ve been looking forward.��But with the two-point goal scored at the Emirates Stadium, there��s little difference between a 3-point draw against each team (1-0) and a 4-1 victory.I felt it when I walked in for the game (in the form of a 4-1 win in which a header by two baskets were enough to close, the match is still relatively easy to beat) and the team��s only goal conceded last the other two. With a 1-0 victory last Thursday, a 2-1 win over the Dynamo lost to Arsenal for the first time in five days – the game was tied 3-1 by a few people. While Arsenal��s second goal showed that it��s more difficult for the team to manage the tournament��s run of scoring with a 7-0-1 on hand in front of the back of the goal, it would be wise to extend this to Arsenal��s last goal of the season before the match.The final goal is not enough to draw the first goal in the 3-1 draw by more than one minute, only the goals a goal have for Arsenal to have an equal playing chance in their games as they had done so previously.Now, they have a great opportunity to get to what they really want and want to feel. These matches are also the main problem, and the key factor in their decision to have the players play, rather than anything else that they wish with a draw or an end-run will be to draw themselves out of the final hurdle and find and play the game for yourself in a mirror moment.This is a game where all these games come into play where everyone has the same experience if they need help or not, and then they can play these on their own-the-road, or as they do for your team. For the last 5-0 game, they are the main reason why not score, or to use their great abilities as their opponents.We��re at least in the situation where we��ll just use this for your team to win two and then beat them. We��re in the game and we want to play it for our team and not lose an advantage in the grand finals.For those who don��t like this tournament, consider this game.In particular, you��ll see them in different games, maybe even if you��re looking at a match, you��ve seen the players play it for the first time. You��d have to play a match for the first couple of minutes to break the law by taking the game by surprise. For players who are playing it most times, however, this is something you wouldn��t want your team to lose and won the game at a tournament. The next time you want to participate and if your teams win it, make it happen next time and you��ve seen them lose the game.We also want to make you see that you might be using our best players�� best players and play them a really bad week or something. We have never seen a team play this weekend with people playing a Dota2 World Cup event for us as a sponsor and so we��ve got an event event for our event to put in a new way.This week we��ve been pushing each other very quickly and are working in collaboration with an artist who will be playing the team, which is going to be a place for us to see. As we work on new things in our team, there��s a special spark-of-the-heart-and-blood with every event that I��ve held for a decade. But the first step up to the next event is the fact that only a quarter, would take in any doubt about it and we already are in the business for us to be able to raise our talents. If we are truly on the scene, do you have to take it from you to help you to do that, as, so many other players, that the artists will know all about their game and they will be happy to have one and they would be okay. The final is about making a team, working for a team, a team that has lost the previous game so they��ll bring them to work together to participate in another world tournament. And the last step up with what does this mean for any one of our team? With so many more people, we always want people to be happy with the success we have. By helping us and making it better, we have achieved this goal of winning the event. It is amazing to have the best that we can deliver.�� –@nkwad.com or: <a-Zakul.com>A.Mihr.com:<a-Zakul@nkw.com> <a-Zakul@nkwad.com> <a-Zakul@nkwad.com:19<a-Zakul@nkwad.com> <a-Boing@nkwad.com:7<a-Zakul@nkwad.com> <a-Zakul@nkwad.com:9<a-Boing@nksham.com:9<a-Zakul@nkwad.com:9<a-Boing@nkwad.com:8<a-Boing@nksham.com:9<a-Zakul@nkwad.com:10<a-Zakul@nkwad.com<a-Zakul@nkwad.com:9<a-Zakul@nkwad.com:9<a-Zakul@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9=0<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@ nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwxada@nkwad.com:9<a-Boing@{nkwad.com:9=0<a-Boing@nkwads.com.9<a-Boing@nkwad.com:9<a-Boing@nksham.com<a-Boing@nkwad.com:9<a-Boing@nksz.com:9<a-Boing@nkwad.com:9<a-Boing@nksmc.com>>a-Boing@nkwad.com<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:99<a-Boing@nkwad.com:9<a-Boing@nkwads.com>;c-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@na.com:9<a b-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a+Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkWad.com:9<a-Boing@nkwad.com:9<a-Boing@nk wad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:7<a-Boing@nkwads.com:9<a-Groing@nkwads.com:9<a-Bling@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-boing@nkWad.com:9<a-Boing@nkwads.com:999<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvalls.com:9<a-Boing@nkneed-tvers.com:12<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:33<a-Boing@nkwads.com:10<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkook.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Bling@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwad@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.comonce upon a time in a distant galaxy 《, we must remember that this new environment must have taken place at the beginning of this first phase of history. Since it is in a world of constant flux, the past and the future are not even close together, but since we are in a world where most of the human civilizations may suffer from the same fate as life itself, we will cease our efforts to be alive at the beginning of this final phase of the human society; the ancient empire that is one of light and darkness in its way can be restored; and the planet that’s darkness and light shining in the heavens! This kind of world is more of a science, an example of the modern era than most human society ever conceived. In the past, we were always imagining more of these new worlds being in the range of the past. The human civilization, in other words, became an order of dark forces of the past: the future and the end of that mind that is the final and final time for us to live. We never think this way, yet we always remember the past, but to that there are still people with whom it will be in the midst of the future, it was not as good in the present moment. This is why we are at the beginning of this new era. We are at what point, the beginning of this new age of humanity’s future, in a world where people of us will look and see the world as in the future; but the experience of our journey could not be taken away, and not, but I also want to say this: for we’re at the beginning of this epoch of civilization- we can all share the same fate. I think that our next generation of our lives will become our “world” in many ways; we can ever know that we have the opportunity to live more as a country or a place to be as good as all else. But we can’t exist. We can’t. And we can have the people who care about us. We can live better when we can and should be treated equally. If we don’t, or we have it, we can’t. And don’t. We can’t, and we can’t, or if we don’t, it should be possible to live on it now. And we can never know how to survive without being on our feet. We can’t, we can’t, but if we’re not, we don’t have it. We can’t live in a world that they can’t, because the government has no moral right to be respected, because we can’t and no human beings can get there. I think we should have our rights. Let the people, and we can keep the money, and the people, we can live free if they can’t, I don’t know what they’ve done so far, but I don’t want to be a man. Maybe you want to be your own brother, so what’s that? Would you do that in your name if I’m going to live life unless I’m in a fight? And I’m just gonna be a man I just don’t know. And I want to be a man, I’m gonna have everything in my life. And then I’ll be the father of his children, all the way to Earth. But I’m gonna be a man to me, and hopefully my family and I’ll be a man to me. And I will be there again. I’ll be in a room with all my children, every night. Let me go, my family’s, the son, and my kids. And I’ll be there again.” And we’ll share a great mutual friend. He’s not afraid of him because of the money he’s having to offer it to him and have him put up there. I felt like my father who’s paying it to him can’t answer this question. I get a little less of an explanation for what’s happening, which is a very good thing, there’s a great deal of love in him, other than more love. So, I don’t know if we’ll ever know it, because I think it’s the same thing as I have a friend and a son because you might want a lot of love, and that’s the real deal, to all his friends, and they do. Then, we’ll have a different story, a relationship with him, each about himself, and we won’t talk about one another—not on the same topic—in the next room or in the next room, with a child, so on. It’s not really important to me, because I should do that and that’s what I do. It’s because I’m a lot of people I feel comfortable with for other purposes. I was a kid who’s very, very protective of what’s going on and so forth.” [He’s] thinking about it over a long time is going to give away. He is being kind of frustrated, it’s because nobody has done it, unless the child is like, “Oh my God, what happens when you’re angry?” We’d be like, “Fuck, I’m in a dream, you know,” we have, so that means that we’re really just seeing different ends and there’s no point in saying that we get to make that relationship. And that’s when it’s time to go away. We’ll come out in a “my brother” and when we’re like, “You’re doing something,” the idea of that moment is the thing I’m watching on the show and I like it because it’s such a little weird. And we’d be very kind of happy to make this way easier in a different way. You can tell that we’ve seen similar ends but the thing about [The Dnyskorn] the way a lot of things are going on are like, “Just see how he’s going.” And then we’ve seen an improvement from, “This is funny.” That’s what he’s doing, and how it can change the mind of what’s going on. And we’ve seen a decrease in the amount of time that’s going on. Just look at the show and see what happens next. And there’s an awful disconnect between two different ends. And you never know how much that’s gonna happen to an actor who’s not the person who’s a person, but you have a different way of doing it. Because for me, it’s the same sort of a situation that’s been so many [laughs]. I’ve gotten it myself. I mean, this has become so much worse. It’s like, “Yeah, I know how much that happened.” It’s a hard time to even learn. I’m not a good person that comes across as the only person in the show. When we talk about it, you see it, “It can’t happen again,” which is really the same story. I don’t have to learn. I’ve heard that, because after reading the first episode, I would never get it. We have to get you back on track at some point just because you’re looking at me like, Why is there something that I think? You’ve been looking forward. It doesn’t turn out the direction I did a couple years ago. [Laughter.] And I’m really interested in thinking about this show, because it looks like it’s going to show when I get back to the end, because if I lost it he’s doing it now. I’m excited because I’ve been getting the season five episode for two decades, but then, I do see that because when I see something big I do it, I’m ready to move on, so I’m ready to do it. I’m gonna do it now, because that would give me that time and have a sense of responsibility.” That’s something that I’m really proud of in its life. It’s been a lot in my life and it’s kind of the best way there’s not a lot of people, you don’t know where it’s going from there. And I think it might be a little bit more of a bit more of a bit more time. Right now my season has a few really good guys. So it’s been fun to watch these people play their games more than once, but I have to be a year and I’m excited to see all the new kid from the back on the couch with the big picture of my kids playing at home. It’s a big deal because I can’t play in my life, or anything like that unless it’s a long play. But the time is finally there and it goes the same way as it was before I can use it. That’s part of why I go this long and that’s actually where I’m most proud of but still.“You have said I need your input and it’s time to build that one or the other,” he said. “Don’t let it go wrong. It’s a good chance for your team to be better and better and better and see the better or win faster as well.” They’ve been more than happy. The same time, on the left there’s a lot of hope about the teams you need. They’ve been a big part of the team around town, and I don’t really know if they’ll want to stay in town like they did before that before the season’s events.“I remember thinking about the game, about 20 of our kids playing against three of us,” he added. “The team in Dallas, I feel really good. I’m thrilled to be a part of our team.”The second of those? Not long after we was announced, it was announced that an agreement was reached, which means we’ll need to pay $18 million to come back to Toronto in the 2013, and that’s a $11 million deal, yet we’ll be happy to see if he can help us make the change and we can say, oh.” That’s exactly what we’ve tried to do. The team doesn’t, and it doesn’t take an extra one’s. One way to say that we’re all on our side is to see the team and the media? Maybe it’s very much about our own history since the start of it.”Meanwhile, on Wednesday, and Monday, they went from having the option, though the club wasn’t happy watching it. It should be expected to be able to use the two-point effort without one of these two-point games on the table to give them the best shot at the championship — and that means that for us that has to be an important step in life for us it’s a great opportunity.“We’ll get a little more time, but I’ve been back with some friends on the board,” said James Binder, the player and the owner of the club. “We’re so excited to see the game, and I’ve been wanting it. I’m looking forward to working with a little bit on my hands,\" says the former player. “And of course, that is a big challenge to have. It’s an important component of the coaching of people and I’ve been looking forward.”But with the two-point goal scored at the Emirates Stadium, there’s little difference between a 3-point draw against each team (1-0) and a 4-1 victory.I felt it when I walked in for the game (in the form of a 4-1 win in which a header by two baskets were enough to close, the match is still relatively easy to beat) and the team’s only goal conceded last the other two. With a 1-0 victory last Thursday, a 2-1 win over the Dynamo lost to Arsenal for the first time in five days – the game was tied 3-1 by a few people. While Arsenal’s second goal showed that it’s more difficult for the team to manage the tournament’s run of scoring with a 7-0-1 on hand in front of the back of the goal, it would be wise to extend this to Arsenal’s last goal of the season before the match.The final goal is not enough to draw the first goal in the 3-1 draw by more than one minute, only the goals a goal have for Arsenal to have an equal playing chance in their games as they had done so previously.Now, they have a great opportunity to get to what they really want and want to feel. These matches are also the main problem, and the key factor in their decision to have the players play, rather than anything else that they wish with a draw or an end-run will be to draw themselves out of the final hurdle and find and play the game for yourself in a mirror moment.This is a game where all these games come into play where everyone has the same experience if they need help or not, and then they can play these on their own-the-road, or as they do for your team. For the last 5-0 game, they are the main reason why not score, or to use their great abilities as their opponents.We’re at least in the situation where we’ll just use this for your team to win two and then beat them. We’re in the game and we want to play it for our team and not lose an advantage in the grand finals.For those who don’t like this tournament, consider this game.In particular, you’ll see them in different games, maybe even if you’re looking at a match, you’ve seen the players play it for the first time. You’d have to play a match for the first couple of minutes to break the law by taking the game by surprise. For players who are playing it most times, however, this is something you wouldn’t want your team to lose and won the game at a tournament. The next time you want to participate and if your teams win it, make it happen next time and you’ve seen them lose the game.We also want to make you see that you might be using our best players’ best players and play them a really bad week or something. We have never seen a team play this weekend with people playing a Dota2 World Cup event for us as a sponsor and so we’ve got an event event for our event to put in a new way.This week we’ve been pushing each other very quickly and are working in collaboration with an artist who will be playing the team, which is going to be a place for us to see. As we work on new things in our team, there’s a special spark-of-the-heart-and-blood with every event that I’ve held for a decade. But the first step up to the next event is the fact that only a quarter, would take in any doubt about it and we already are in the business for us to be able to raise our talents. If we are truly on the scene, do you have to take it from you to help you to do that, as, so many other players, that the artists will know all about their game and they will be happy to have one and they would be okay. The final is about making a team, working for a team, a team that has lost the previous game so they’ll bring them to work together to participate in another world tournament. And the last step up with what does this mean for any one of our team? With so many more people, we always want people to be happy with the success we have. By helping us and making it better, we have achieved this goal of winning the event. It is amazing to have the best that we can deliver.” –@nkwad.com or: <a-Zakul.com>A.Mihr.com:<a-Zakul@nkw.com> <a-Zakul@nkwad.com> <a-Zakul@nkwad.com:19<a-Zakul@nkwad.com> <a-Boing@nkwad.com:7<a-Zakul@nkwad.com> <a-Zakul@nkwad.com:9<a-Boing@nksham.com:9<a-Zakul@nkwad.com:9<a-Boing@nkwad.com:8<a-Boing@nksham.com:9<a-Zakul@nkwad.com:10<a-Zakul@nkwad.com<a-Zakul@nkwad.com:9<a-Zakul@nkwad.com:9<a-Zakul@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9=0<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@ nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwxada@nkwad.com:9<a-Boing@{nkwad.com:9=0<a-Boing@nkwads.com.9<a-Boing@nkwad.com:9<a-Boing@nksham.com<a-Boing@nkwad.com:9<a-Boing@nksz.com:9<a-Boing@nkwad.com:9<a-Boing@nksmc.com>>a-Boing@nkwad.com<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:99<a-Boing@nkwad.com:9<a-Boing@nkwads.com>;c-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@na.com:9<a b-Boing@nkwad.com:9<a-Boing@nkwad.com:9<a+Boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-boing@nkwad.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkWad.com:9<a-Boing@nkwad.com:9<a-Boing@nk wad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:7<a-Boing@nkwads.com:9<a-Groing@nkwads.com:9<a-Bling@nkwads.com:9<a-Boing@nkwad.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-boing@nkWad.com:9<a-Boing@nkwads.com:999<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvalls.com:9<a-Boing@nkneed-tvers.com:12<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:33<a-Boing@nkwads.com:10<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkook.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkwads.com:9<a-Bling@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkneed-tvers.com:9<a-Boing@nkwad@nkwads.com:9<a-Boing@nkwads.com:9<a-Boing@nkneed-tvers.com\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "text_to_be_continued = \"once upon a time in a distant galaxy \"\n",
    "context = torch.tensor(encode(text_to_be_continued), dtype=torch.long, device=device).unsqueeze(0)\n",
    "print(decode(model.generate(context, max_new_tokens=5000, print_characters=True)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755928fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
